{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Much of the code for this notebook is sourced from the original underlying base model scCraft. This ensures reproducibilty of the original results in comparison to the new approach. The github link is available here: https://github.com/ch2343/scCRAFT/tree/main, and a link to the publication is available here: https://www.nature.com/articles/s42003-025-07988-y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scib.utils import *\n",
    "import torch\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "import torch.optim as optim\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from torch.autograd import grad\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import kl_divergence as kl\n",
    "\n",
    "from itertools import combinations\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy\n",
    "import scib\n",
    "import time\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # Skip if it's an instance of _DomainSpecificBatchNorm\n",
    "    if classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def generate_balanced_dataloader(adata, batch_size, batch_key=\"batch\"):\n",
    "    if not adata.obs_names.is_unique:\n",
    "        print(\"Error: Indices are not unique!\")\n",
    "        raise AssertionError(\n",
    "            \"Indices are not unique. Please ensure the indices are unique before proceeding.\"\n",
    "        )\n",
    "    # Map unique batch keys to integers\n",
    "    unique_batches = adata.obs[batch_key].unique()\n",
    "    batch_to_int = {batch: i for i, batch in enumerate(unique_batches)}\n",
    "    unsupervised_labels1 = adata.obs[\"leiden1\"].cat.codes.values\n",
    "    unsupervised_labels2 = adata.obs[\"leiden2\"].cat.codes.values\n",
    "    # Separate the dataset by batches and sample indices\n",
    "    batch_indices = []\n",
    "    batch_labels_list = []\n",
    "    for batch in unique_batches:\n",
    "        # Find the indices for the current batch\n",
    "        batch_indices_in_adata = adata.obs[adata.obs[batch_key] == batch].index\n",
    "\n",
    "        # Sample indices from the current batch\n",
    "        if len(batch_indices_in_adata) >= batch_size:\n",
    "            sampled_indices = np.random.choice(batch_indices_in_adata, batch_size, replace=False)\n",
    "        else:\n",
    "            # If not enough cells, sample with replacement\n",
    "            sampled_indices = np.random.choice(batch_indices_in_adata, batch_size, replace=True)\n",
    "\n",
    "        # Get the integer positions of the sampled indices\n",
    "        sampled_indices_pos = [adata.obs_names.get_loc(idx) for idx in sampled_indices]\n",
    "        batch_indices.extend(sampled_indices_pos)\n",
    "\n",
    "        # Map the batch keys to integers and add to the label list\n",
    "        batch_labels_list.extend([batch_to_int[batch]] * batch_size)\n",
    "\n",
    "    # Extract the feature data\n",
    "    x_sampled = adata.X[batch_indices, :]\n",
    "\n",
    "    # Convert features to tensor\n",
    "    if isinstance(x_sampled, np.ndarray):\n",
    "        x_tensor = torch.tensor(x_sampled, dtype=torch.float32)\n",
    "    else:  # if it's a sparse matrix\n",
    "        x_tensor = torch.tensor(x_tensor.toarray(), dtype=torch.float32)\n",
    "\n",
    "    # Convert batch labels to tensor\n",
    "    v_tensor = torch.tensor(batch_labels_list, dtype=torch.int64)\n",
    "    label_tensor1 = torch.tensor(unsupervised_labels1[batch_indices], dtype=torch.int64)\n",
    "    label_tensor2 = torch.tensor(unsupervised_labels2[batch_indices], dtype=torch.int64)\n",
    "\n",
    "    # Create a TensorDataset and DataLoader\n",
    "    combined_dataset = TensorDataset(x_tensor, v_tensor, label_tensor1, label_tensor2)\n",
    "    dataloader = DataLoader(combined_dataset, batch_size=batch_size * 2, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# Example usage (assuming `adata` is your AnnData object):\n",
    "# data_loader = generate_balanced_dataloader(adata, batch_size=256, batch_key='batch')\n",
    "\n",
    "\n",
    "def count_labels_per_batch(labels, batch_ids):\n",
    "    unique_batches = batch_ids.unique()\n",
    "    label_counts_per_batch = {\n",
    "        batch: (\n",
    "            labels[batch_ids == batch].unique(),\n",
    "            torch.stack(\n",
    "                [\n",
    "                    (labels[batch_ids == batch] == label).sum()\n",
    "                    for label in labels[batch_ids == batch].unique()\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        for batch in unique_batches\n",
    "    }\n",
    "    return label_counts_per_batch\n",
    "\n",
    "\n",
    "def create_triplets(\n",
    "    embeddings, labels, labels_high, batch_ids, margin=1.0, num_triplets_per_label=15\n",
    "):\n",
    "    label_counts_per_batch = count_labels_per_batch(labels, batch_ids)\n",
    "    triplets = []\n",
    "\n",
    "    for batch_id, (unique_labels, _) in label_counts_per_batch.items():\n",
    "        for label in unique_labels:\n",
    "            indices_with_label = (labels == label) & (batch_ids == batch_id)\n",
    "            indices_without_label = (labels != label) & (batch_ids == batch_id)\n",
    "\n",
    "            positive_pairs = list(combinations(torch.where(indices_with_label)[0], 2))\n",
    "            negative_indices = torch.where(indices_without_label)[0]\n",
    "\n",
    "            # Randomly sample triplets\n",
    "            sampled_positive_pairs = random.sample(\n",
    "                positive_pairs, min(num_triplets_per_label, len(positive_pairs))\n",
    "            )\n",
    "            # Ensure the number of negative samples doesn't exceed the available negatives\n",
    "            num_negative_samples = min(len(sampled_positive_pairs), len(negative_indices))\n",
    "            sampled_negative_indices = random.sample(list(negative_indices), num_negative_samples)\n",
    "\n",
    "            for (anchor_idx, positive_idx), negative_idx in zip(\n",
    "                sampled_positive_pairs, sampled_negative_indices\n",
    "            ):\n",
    "                anchor, positive, negative = (\n",
    "                    embeddings[anchor_idx],\n",
    "                    embeddings[positive_idx],\n",
    "                    embeddings[negative_idx],\n",
    "                )\n",
    "                if labels_high[anchor_idx] != labels_high[positive_idx]:\n",
    "                    # Only consider anchor-negative pair for loss calculation\n",
    "                    # triplet_loss = torch.relu(-torch.norm(anchor - negative) + margin)\n",
    "                    continue\n",
    "                else:\n",
    "                    # Standard triplet loss calculation for anchor-positive and anchor-negative\n",
    "                    triplet_loss = torch.relu(\n",
    "                        torch.norm(anchor - positive) - torch.norm(anchor - negative) + margin\n",
    "                    )\n",
    "                    triplets.append(triplet_loss)\n",
    "\n",
    "    if triplets:\n",
    "        return torch.mean(torch.stack(triplets))\n",
    "    else:\n",
    "        return torch.tensor(0.0)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the seed for different packages to ensure reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    seed (int): The seed number.\n",
    "    \"\"\"\n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "\n",
    "def multi_resolution_cluster(adata, resolution1=0.5, resolution2=7, method=\"Louvain\"):\n",
    "    \"\"\"\n",
    "    Performs PCA, neighbors calculation, and clustering with specified resolutions and method.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing single-cell data.\n",
    "    - resolution1: float, the resolution parameter for the first clustering.\n",
    "    - resolution2: float, the resolution parameter for the second clustering.\n",
    "    - method: str, clustering method to use (\"Louvain\" or \"Leiden\").\n",
    "\n",
    "    The function updates `adata` in place, adding two new columns to `adata.obs`:\n",
    "    - 'leiden1': contains cluster labels from the first clustering.\n",
    "    - 'leiden2': contains cluster labels from the second clustering.\n",
    "    \"\"\"\n",
    "    # Perform PCA\n",
    "    sc.tl.pca(adata, n_comps=50)\n",
    "    # Compute neighbors using the PCA representation\n",
    "    sc.pp.neighbors(adata, use_rep=\"X_pca\")\n",
    "\n",
    "    # Determine the clustering function based on the method\n",
    "    if method.lower() == \"louvain\":\n",
    "        clustering_function = sc.tl.louvain\n",
    "    elif method.lower() == \"leiden\":\n",
    "        clustering_function = sc.tl.leiden\n",
    "    else:\n",
    "        raise ValueError(\"Method should be 'Louvain' or 'Leiden'\")\n",
    "\n",
    "    # Perform the first round of clustering\n",
    "    clustering_function(adata, resolution=resolution1)\n",
    "    adata.obs[\"leiden1\"] = adata.obs[method.lower()]\n",
    "\n",
    "    # Perform the second round of clustering with a different resolution\n",
    "    clustering_function(adata, resolution=resolution2)\n",
    "    adata.obs[\"leiden2\"] = adata.obs[method.lower()]\n",
    "\n",
    "\n",
    "# Loader only for Visualization\n",
    "def generate_adata_to_dataloader(adata, batch_size=2048):\n",
    "    x_dense = adata.X.toarray() if isinstance(adata.X, scipy.sparse.spmatrix) else adata.X\n",
    "\n",
    "    x_tensor = torch.tensor(x_dense, dtype=torch.float32)\n",
    "\n",
    "    # Create a DataLoader for batch-wise processing\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        x_tensor, torch.arange(len(x_tensor))\n",
    "    )  # include indices\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + Loss function\n",
    "\n",
    "def log_nb_positive(\n",
    "    x: Union[torch.Tensor, jnp.ndarray],\n",
    "    mu: Union[torch.Tensor, jnp.ndarray],\n",
    "    theta: Union[torch.Tensor, jnp.ndarray],\n",
    "    eps: float = 1e-8,\n",
    "    log_fn: callable = torch.log,\n",
    "    lgamma_fn: callable = torch.lgamma,\n",
    "):\n",
    "    \"\"\"Log likelihood (scalar) of a minibatch according to a nb model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        data\n",
    "    mu\n",
    "        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)\n",
    "    theta\n",
    "        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)\n",
    "    eps\n",
    "        numerical stability constant\n",
    "    log_fn\n",
    "        log function\n",
    "        lgamma_fn\n",
    "        log gamma function\n",
    "    \"\"\"\n",
    "    log = log_fn\n",
    "    lgamma = lgamma_fn\n",
    "    log_theta_mu_eps = log(theta + mu + eps)\n",
    "    res = (\n",
    "        theta * (log(theta + eps) - log_theta_mu_eps)\n",
    "        + x * (log(mu + eps) - log_theta_mu_eps)\n",
    "        + lgamma(x + theta)\n",
    "        - lgamma(theta)\n",
    "        - lgamma(x + 1)\n",
    "    )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def reparameterize_gaussian(mu, var):\n",
    "    return Normal(mu, var.sqrt()).rsample()\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, p_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # Define the architecture\n",
    "        self.fc1 = nn.Linear(p_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc_mean = nn.Linear(512, latent_dim)  # Output layer for mean\n",
    "        self.fc_var = nn.Linear(512, latent_dim)  # Output layer for variance\n",
    "        # self.fc_library = nn.Linear(512, 1)        # Output layer for library size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "    def forward(self, x, warmup):\n",
    "        # Forward pass through the network\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(self.bn1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(self.bn2(x))\n",
    "\n",
    "        # Separate paths for mean, variance, and library size\n",
    "        q_m = self.fc_mean(x)\n",
    "        q_v = torch.exp(self.fc_var(x)) + 1e-4\n",
    "        # library = self.fc_library(x)  # Predicted log library size\n",
    "\n",
    "        z = reparameterize_gaussian(q_m, q_v)\n",
    "\n",
    "        return q_m, q_v, z\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, p_dim, v_dim, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Main decoder pathway\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + v_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, p_dim),\n",
    "        )\n",
    "\n",
    "        # Additional pathway for the batch effect (ec)\n",
    "        self.decoder_ec = nn.Sequential(\n",
    "            nn.Linear(v_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, p_dim),\n",
    "        )\n",
    "\n",
    "        # Parameters for ZINB distribution\n",
    "        self.px_scale_decoder = nn.Linear(p_dim, p_dim)  # mean (rate) of ZINB\n",
    "        self.px_r_decoder = nn.Linear(p_dim, p_dim)  # dispersion\n",
    "\n",
    "    def forward(self, z, ec):\n",
    "        # Main decoding\n",
    "        z_ec = torch.cat((z, ec), dim=-1)\n",
    "        decoded = self.decoder(z_ec)\n",
    "        decoded_ec = self.decoder_ec(ec)\n",
    "\n",
    "        # Combining outputs\n",
    "        combined = self.relu(decoded + decoded_ec)\n",
    "\n",
    "        # NB parameters with safe exponential\n",
    "\n",
    "        px_scale = torch.exp(self.px_scale_decoder(combined))\n",
    "        px_r = torch.exp(self.px_r_decoder(combined))\n",
    "\n",
    "        # Scale the mean (px_scale) with the predicted library size\n",
    "        px_rate = px_scale\n",
    "\n",
    "        return px_rate, px_r\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, p_dim, v_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(p_dim, latent_dim)\n",
    "        self.decoder = Decoder(p_dim, v_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, ec, warmup):\n",
    "        # Encoding\n",
    "        q_m, q_v, z = self.encoder(x, warmup)\n",
    "\n",
    "        # Decoding\n",
    "        px_scale, px_r = self.decoder(z, ec)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        # reconst_loss = F.mse_loss(px_scale, x)\n",
    "        reconst_loss = -log_nb_positive(x, px_scale, px_r)\n",
    "        # KL Divergence\n",
    "        mean = torch.zeros_like(q_m)\n",
    "        scale = torch.ones_like(q_v)\n",
    "        kl_divergence = kl(Normal(q_m, torch.sqrt(q_v)), Normal(mean, scale)).sum(dim=1)\n",
    "\n",
    "        return reconst_loss, kl_divergence, z, px_scale\n",
    "\n",
    "\n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Apply log softmax to the output\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        # Compute the negative log likelihood loss\n",
    "        loss = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class WassersteinLoss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, batch_ids):\n",
    "        source = output[batch_ids != 0]\n",
    "        target = output[batch_ids == 0]\n",
    "\n",
    "        # Compute the Wasserstein loss\n",
    "        loss = -1 * target.mean() + source.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MultiClassWassersteinLoss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, batch_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output: Tensor of shape [B, K] - critic scores for each class.\n",
    "            batch_ids: Tensor of shape [B] - true domain IDs (0 to K-1).\n",
    "        Returns:\n",
    "            Wasserstein loss encouraging domain confusion.\n",
    "        \"\"\"\n",
    "        num_domains = output.shape[1]  # number of classes/domains\n",
    "        loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        for k in range(num_domains):\n",
    "            mask_k = batch_ids == k\n",
    "            if mask_k.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            # Scores for domain k samples from the k-th output head\n",
    "            d_kk = output[mask_k, k]  # true class head for samples from domain k\n",
    "\n",
    "            # Scores for domain k samples from all other heads\n",
    "            d_kj = output[mask_k]  # [n_k, K]\n",
    "            mask_other = torch.ones(num_domains, dtype=torch.bool, device=output.device)\n",
    "            mask_other[k] = False\n",
    "            d_kj_others = d_kj[:, mask_other]  # [n_k, K-1]\n",
    "\n",
    "            # Wasserstein-style loss: true score - mean of other scores\n",
    "            diff = d_kk.mean() - d_kj_others.mean()\n",
    "            loss += diff\n",
    "            total += 1\n",
    "\n",
    "        loss = loss / total\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def gradient_penalty(discriminator, real_samples, fake_samples, device=\"cpu\"):\n",
    "    \"\"\"Computes gradient penalty for WGAN-GP\"\"\"\n",
    "    batch_size = real_samples.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, device=device)\n",
    "    epsilon = epsilon.expand_as(real_samples)\n",
    "\n",
    "    # if there is a mismatch in shape, subsample the larger tensor\n",
    "    if fake_samples.shape != real_samples.shape:\n",
    "        if fake_samples.shape[0] > real_samples.shape[0]:\n",
    "            perm = torch.randperm(fake_samples.shape[0], device=device)\n",
    "            fake_samples = fake_samples[perm[: real_samples.shape[0]]]\n",
    "        elif real_samples.shape[0] > fake_samples.shape[0]:\n",
    "            perm = torch.randperm(real_samples.shape[0], device=device)\n",
    "            real_samples = real_samples[perm[: fake_samples.shape[0]]]\n",
    "\n",
    "    # Interpolate between real and fake samples\n",
    "    interpolated = epsilon * real_samples + (1 - epsilon) * fake_samples\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    # Forward pass\n",
    "    d_interpolated = discriminator(interpolated, None, generator=True)\n",
    "\n",
    "    # Forcing scalar output if necessary\n",
    "    if d_interpolated.dim() > 1:\n",
    "        d_interpolated = d_interpolated.view(-1)\n",
    "\n",
    "    # Compute gradients w.r.t. interpolated\n",
    "    gradients = grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_interpolated),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    # Compute the gradient norm\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "\n",
    "    # Compute the penalty\n",
    "    penalty = ((grad_norm - 1) ** 2).mean()\n",
    "    return penalty\n",
    "\n",
    "\n",
    "def multi_class_gradient_penalty(critic, z, batch_ids, lambda_gp=10.0):\n",
    "    \"\"\"\n",
    "    Computes the multi-class gradient penalty for a multi-output critic.\n",
    "\n",
    "    Args:\n",
    "        critic: A callable that maps latent vectors z to shape [B, K] (K = num domains).\n",
    "        z: Latent vectors, shape [B, latent_dim].\n",
    "        batch_ids: Tensor of shape [B] with domain labels (0 to K-1).\n",
    "        lambda_gp: Weight of gradient penalty.\n",
    "\n",
    "    Returns:\n",
    "        Scalar gradient penalty loss.\n",
    "    \"\"\"\n",
    "    b, latent_dim = z.shape\n",
    "    critic_out = critic(z, batch_ids=None).shape[1]\n",
    "    gp_total = 0.0\n",
    "    device = z.device\n",
    "    total_classes = 0\n",
    "\n",
    "    for k in range(critic_out):\n",
    "        # Get samples from domain k\n",
    "        mask_k = batch_ids == k\n",
    "        if mask_k.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        z_k = z[mask_k]\n",
    "        z_ref = z[torch.randperm(z.size(0))[: z_k.size(0)]]\n",
    "\n",
    "        # Interpolate\n",
    "        epsilon = torch.rand(z_k.size(0), 1, device=device)\n",
    "        epsilon = epsilon.expand_as(z_k)\n",
    "        z_hat = epsilon * z_k + (1 - epsilon) * z_ref\n",
    "        z_hat.requires_grad_(True)\n",
    "\n",
    "        # Forward pass through critic\n",
    "        out = critic(z_hat, batch_ids=None)  # [B_k, K]\n",
    "        out_k = out[:, k].sum()  # Only the k-th head\n",
    "\n",
    "        # Compute gradients\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=out_k, inputs=z_hat, create_graph=True, retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        # Compute L2 norm of gradients\n",
    "        grad_norm = grad.view(grad.size(0), -1).norm(2, dim=1)\n",
    "        gp = ((grad_norm - 1) ** 2).mean()\n",
    "\n",
    "        gp_total += gp\n",
    "        total_classes += 1\n",
    "\n",
    "    if total_classes == 0:\n",
    "        return torch.tensor(0.0, device=z.device)\n",
    "\n",
    "    return lambda_gp * gp_total / total_classes\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_input, domain_number, critic=False):\n",
    "        super().__init__()\n",
    "        n_hidden = 128\n",
    "        self.critic = critic\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, domain_number)\n",
    "\n",
    "        if self.critic:\n",
    "            # If using critic, use Wasserstein loss\n",
    "            self.loss = MultiClassWassersteinLoss()\n",
    "        else:\n",
    "            # If not using critic, use cross-entropy loss\n",
    "            self.loss = CrossEntropy()\n",
    "\n",
    "    def forward(self, x, batch_ids, generator=False):\n",
    "        # Forward pass through layers\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        output = self.fc3(h)\n",
    "\n",
    "        if batch_ids is None:\n",
    "            # If batch_ids is None, return the output directly\n",
    "            return output\n",
    "\n",
    "        discriminator_loss = self.loss(output, batch_ids)\n",
    "\n",
    "        gp_loss = 0.0\n",
    "\n",
    "        if self.loss.reduction == \"mean\":\n",
    "            discriminator_loss = discriminator_loss.mean()\n",
    "        elif self.loss.reduction == \"sum\":\n",
    "            discriminator_loss = discriminator_loss.sum()\n",
    "        if self.critic:\n",
    "            gp_loss = multi_class_gradient_penalty(self, x, batch_ids)\n",
    "\n",
    "        return discriminator_loss, gp_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training class\n",
    "class SCIntegrationModel(nn.Module):\n",
    "    def __init__(self, adata, batch_key, z_dim, critic, seed):\n",
    "        super().__init__()\n",
    "        self.p_dim = adata.shape[1]\n",
    "        self.z_dim = z_dim\n",
    "        self.v_dim = np.unique(adata.obs[batch_key]).shape[0]\n",
    "\n",
    "        # Correctly initialize VAE with p_dim, v_dim, and latent_dim\n",
    "        self.VAE = VAE(p_dim=self.p_dim, v_dim=self.v_dim, latent_dim=self.z_dim)\n",
    "        self.D_Z = Discriminator(n_input=self.z_dim, domain_number=self.v_dim, critic=critic)\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "        # self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = \"cpu\"\n",
    "        # Move models to CUDA if available\n",
    "        self.VAE.to(self.device)\n",
    "        self.D_Z.to(self.device)\n",
    "\n",
    "        # Initialize weights\n",
    "        if seed is not None:\n",
    "            set_seed(seed)\n",
    "        self.VAE.apply(weights_init_normal)\n",
    "        self.D_Z.apply(weights_init_normal)\n",
    "\n",
    "    def train_model(\n",
    "        self,\n",
    "        adata,\n",
    "        batch_key,\n",
    "        epochs,\n",
    "        d_coef,\n",
    "        kl_coef,\n",
    "        triplet_coef,\n",
    "        cos_coef,\n",
    "        warmup_epoch,\n",
    "        disc_iter,\n",
    "    ):\n",
    "        # Optimizer for VAE (Encoder + Decoder)\n",
    "        optimizer_g = optim.Adam(self.VAE.parameters(), lr=0.001, weight_decay=0.0)\n",
    "        # Optimizer for Discriminator\n",
    "        optimizer_d_z = optim.Adam(self.D_Z.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            data_loader = generate_balanced_dataloader(adata, batch_size=512, batch_key=batch_key)\n",
    "            self.VAE.train()\n",
    "            self.D_Z.train()\n",
    "            all_losses = 0\n",
    "            d_loss = 0\n",
    "            t_loss = 0\n",
    "            v_loss = 0\n",
    "            warmup = epoch < warmup_epoch\n",
    "            for _, (x, v, labels_low, labels_high) in enumerate(data_loader):\n",
    "                x = x.to(self.device)\n",
    "                v = v.to(self.device)\n",
    "                labels_low = labels_low.to(self.device)\n",
    "                labels_high = labels_high.to(self.device)\n",
    "                batch_size = x.size(0)\n",
    "                v_true = v\n",
    "                v_one_hot = torch.zeros(batch_size, self.v_dim).to(x.device)\n",
    "                # Use scatter_ to put 1s in the indices indicated by v\n",
    "                v = v.unsqueeze(1)  # Ensure v is of shape [batch_size, 1] if it's not already\n",
    "\n",
    "                v_one_hot.scatter_(1, v, 1).to(v.device)\n",
    "\n",
    "                reconst_loss, kl_divergence, z, x_tilde = self.VAE(x, v_one_hot, warmup)\n",
    "                reconst_loss = torch.clamp(reconst_loss, max=1e5)\n",
    "\n",
    "                loss_cos = (\n",
    "                    1 - torch.sum(F.normalize(x_tilde, p=2) * F.normalize(x, p=2), 1)\n",
    "                ).mean()\n",
    "                loss_vae = torch.mean(reconst_loss.mean() + kl_coef * kl_divergence.mean())\n",
    "\n",
    "                for _ in range(disc_iter):\n",
    "                    optimizer_d_z.zero_grad()\n",
    "                    if self.D_Z.critic:\n",
    "                        loss_d_z, gp = self.D_Z(z.detach(), v_true)\n",
    "                    else:\n",
    "                        loss_d_z, gp = self.D_Z(z.detach(), v_true)\n",
    "\n",
    "                    loss_d_z += gp\n",
    "\n",
    "                    loss_d_z.backward(retain_graph=True)\n",
    "                    optimizer_d_z.step()\n",
    "\n",
    "                optimizer_g.zero_grad()\n",
    "                if self.D_Z.critic:\n",
    "                    loss_da, gp = self.D_Z(z, v_true)\n",
    "                else:\n",
    "                    loss_da, gp = self.D_Z(z, v_true)\n",
    "\n",
    "                triplet_loss = create_triplets(z, labels_low, labels_high, v_true, margin=5)\n",
    "\n",
    "                if warmup:\n",
    "                    all_loss = (\n",
    "                        -0 * loss_da\n",
    "                        + 1 * loss_vae\n",
    "                        + gp\n",
    "                        + triplet_coef * triplet_loss\n",
    "                        + cos_coef * loss_cos\n",
    "                    )\n",
    "                else:\n",
    "                    all_loss = (\n",
    "                        -d_coef * loss_da\n",
    "                        + 1 * loss_vae\n",
    "                        + gp\n",
    "                        + triplet_coef * triplet_loss\n",
    "                        + cos_coef * loss_cos\n",
    "                    )\n",
    "\n",
    "                all_loss.backward()\n",
    "                optimizer_g.step()\n",
    "                all_losses += all_loss\n",
    "                d_loss += loss_da\n",
    "                t_loss += triplet_loss\n",
    "                v_loss += loss_vae\n",
    "\n",
    "\n",
    "def train_integration_model(\n",
    "    adata,\n",
    "    disc_iter,\n",
    "    batch_key=\"batch\",\n",
    "    z_dim=256,\n",
    "    epochs=150,\n",
    "    d_coef=0.2,\n",
    "    kl_coef=0.005,\n",
    "    triplet_coef=1,\n",
    "    cos_coef=20,\n",
    "    warmup_epoch=50,\n",
    "    critic=False,\n",
    "    scale=None,\n",
    "):\n",
    "    number_of_cells = adata.n_obs\n",
    "    number_of_batches = np.unique(adata.obs[batch_key]).shape[0]\n",
    "\n",
    "    # Default number of epochs\n",
    "    if epochs == 150:\n",
    "        # Check if the number of cells goes above 100000\n",
    "        if number_of_cells > 100000:\n",
    "            calculated_epochs = int(1.5 * number_of_cells / (number_of_batches * 512))\n",
    "            # If the calculated value is larger than the default, use it instead\n",
    "            if calculated_epochs > epochs:\n",
    "                epochs = calculated_epochs\n",
    "    else:\n",
    "        epochs = epochs\n",
    "    model = SCIntegrationModel(\n",
    "        adata=adata, batch_key=batch_key, z_dim=z_dim, critic=critic, seed=42)\n",
    "    print(epochs)\n",
    "    start_time = time.time()\n",
    "    model.train_model(\n",
    "        adata,\n",
    "        batch_key=batch_key,\n",
    "        epochs=epochs,\n",
    "        d_coef=d_coef,\n",
    "        kl_coef=kl_coef,\n",
    "        triplet_coef=triplet_coef,\n",
    "        cos_coef=cos_coef,\n",
    "        warmup_epoch=warmup_epoch,\n",
    "        disc_iter=disc_iter,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    model.VAE.eval()\n",
    "    return model.VAE\n",
    "\n",
    "\n",
    "def obtain_embeddings(adata, vae, dim=50, pca=True, seed=None):\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    vae.eval()\n",
    "    data_loader = generate_adata_to_dataloader(adata)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_z = []\n",
    "    all_indices = []\n",
    "\n",
    "    for _, (x, indices) in enumerate(data_loader):\n",
    "        x = x.to(device)\n",
    "        _, _, z = vae.encoder(x, warmup=False)\n",
    "        all_z.append(z)\n",
    "        all_indices.extend(indices.tolist())\n",
    "\n",
    "    all_z_combined = torch.cat(all_z, dim=0)\n",
    "    all_indices_tensor = torch.tensor(all_indices)\n",
    "    all_z_reordered = all_z_combined[all_indices_tensor.argsort()]\n",
    "    all_z_np = all_z_reordered.cpu().detach().numpy()\n",
    "\n",
    "    # Create anndata object with reordered embeddings\n",
    "    adata.obsm[\"X_scCRAFT\"] = all_z_np\n",
    "\n",
    "    if pca:\n",
    "        pca_model = PCA(n_components=dim)\n",
    "        # Fit and transform the data\n",
    "        x_sccraft_pca = pca_model.fit_transform(adata.obsm[\"X_scCRAFT\"])\n",
    "        # Store the PCA-reduced data back into adata.obsm\n",
    "        adata.obsm[\"X_scCRAFT\"] = x_sccraft_pca\n",
    "\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_by_technology(adata, batch_key='tech', color_key='celltype', ncols=3, figsize_per_panel=(5, 5)):\n",
    "    \"\"\"\n",
    "    Plot UMAP with consistent x and y scales and consistent colors for each technology/batch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Annotated data object with UMAP coordinates in obsm['X_umap']\n",
    "    batch_key : str, default 'tech'\n",
    "        Key in adata.obs containing batch/technology information\n",
    "    color_key : str, default 'celltype'\n",
    "        Key in adata.obs for coloring points\n",
    "    ncols : int, default 3\n",
    "        Maximum number of columns in subplot grid\n",
    "    figsize_per_panel : tuple, default (5, 5)\n",
    "        Size of each subplot panel\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plots)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    sc.tl.umap(adata, min_dist=0.5)\n",
    "    \n",
    "    # Ensure cell types are categorical\n",
    "    if not pd.api.types.is_categorical_dtype(adata.obs[color_key]):\n",
    "        adata.obs[color_key] = adata.obs[color_key].astype('category')\n",
    "    \n",
    "    # Get unique technologies/batches and cell types\n",
    "    technologies = adata.obs[batch_key].unique()\n",
    "    cell_types = adata.obs[color_key].cat.categories\n",
    "    \n",
    "    # Create a consistent colormap for cell types\n",
    "    cmap = plt.cm.get_cmap('tab20', len(cell_types))\n",
    "    colors = [cmap(i) for i in range(len(cell_types))]\n",
    "    color_dict = dict(zip(cell_types, colors))\n",
    "    \n",
    "    # Get the overall x and y limits from the full UMAP\n",
    "    x_coords = adata.obsm['X_umap'][:, 0]\n",
    "    y_coords = adata.obsm['X_umap'][:, 1]\n",
    "    x_min, x_max = x_coords.min() - 0.5, x_coords.max() + 0.5\n",
    "    y_min, y_max = y_coords.min() - 0.5, y_coords.max() + 0.5\n",
    "    \n",
    "    # Create subplots - adjust the number of columns based on preference\n",
    "    n_techs = len(technologies)\n",
    "    ncols = min(ncols, n_techs)\n",
    "    nrows = (n_techs + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(figsize_per_panel[0]*ncols, figsize_per_panel[1]*nrows))\n",
    "    \n",
    "    # Handle single subplot case\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = [axes]\n",
    "    elif nrows == 1 or ncols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Plot each technology separately\n",
    "    for i, tech in enumerate(technologies):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Subset data for this technology\n",
    "        tech_mask = adata.obs[batch_key] == tech\n",
    "        tech_coords = adata.obsm['X_umap'][tech_mask]\n",
    "        tech_celltypes = adata.obs.loc[tech_mask, color_key]\n",
    "        \n",
    "        # Plot each cell type with consistent colors\n",
    "        for cell_type in cell_types:\n",
    "            cell_mask = tech_celltypes == cell_type\n",
    "            if np.sum(cell_mask) > 0:  # Only plot if there are cells of this type\n",
    "                ax.scatter(\n",
    "                    tech_coords[cell_mask, 0], \n",
    "                    tech_coords[cell_mask, 1],\n",
    "                    color=color_dict[cell_type],\n",
    "                    s=1, alpha=0.7, label=cell_type\n",
    "                )\n",
    "        \n",
    "        # Set consistent limits for all subplots\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('UMAP1')\n",
    "        ax.set_ylabel('UMAP2')\n",
    "        ax.set_title(f'{batch_key.capitalize()}: {tech}')\n",
    "        ax.set_aspect('equal')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_techs, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Add a legend to the figure (outside the plots)\n",
    "    handles, labels = [], []\n",
    "    for cell_type in cell_types:\n",
    "        handles.append(plt.Line2D([0], [0], marker='o', color=color_dict[cell_type], \n",
    "                                 label=cell_type, markersize=5, linestyle='None'))\n",
    "        labels.append(cell_type)\n",
    "    \n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make room for legend\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create a combined plot with all technologies\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "    \n",
    "    # Plot colored by technology/batch\n",
    "    sc.pl.umap(adata, color=batch_key, ax=ax1, frameon=False, show=False)\n",
    "    ax1.set_xlim(x_min, x_max)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    ax1.set_title(f'Colored by {batch_key.capitalize()}')\n",
    "    \n",
    "    # Plot colored by celltype with consistent colors\n",
    "    for cell_type in cell_types:\n",
    "        cell_mask = adata.obs[color_key] == cell_type\n",
    "        if np.sum(cell_mask) > 0:\n",
    "            ax2.scatter(\n",
    "                adata.obsm['X_umap'][cell_mask, 0],\n",
    "                adata.obsm['X_umap'][cell_mask, 1],\n",
    "                color=color_dict[cell_type],\n",
    "                s=1, alpha=0.7, label=cell_type\n",
    "            )\n",
    "    \n",
    "    ax2.set_xlim(x_min, x_max)\n",
    "    ax2.set_ylim(y_min, y_max)\n",
    "    ax2.set_title(f'Colored by {color_key.capitalize()}')\n",
    "    \n",
    "    # Add legend to the second plot\n",
    "    handles, labels = [], []\n",
    "    for cell_type in cell_types:\n",
    "        handles.append(plt.Line2D([0], [0], marker='o', color=color_dict[cell_type], \n",
    "                                 label=cell_type, markersize=5, linestyle='None'))\n",
    "        labels.append(cell_type)\n",
    "    \n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust layout to make room for legend\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lisi(X, metadata, label_colname, perplexity=30):\n",
    "    \"\"\"\n",
    "    Compute Local Inverse Simpson Index (LISI) for batch mixing evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The embedded data matrix\n",
    "    metadata : pandas.DataFrame\n",
    "        Metadata containing batch/label information\n",
    "    label_colname : str\n",
    "        Column name in metadata containing the batch labels\n",
    "    perplexity : int, default=30\n",
    "        Perplexity parameter for Gaussian kernel (similar to t-SNE)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    lisi_scores : array-like\n",
    "        LISI score for each cell\n",
    "    \"\"\"\n",
    "    n_cells = X.shape[0]\n",
    "    \n",
    "    # Get batch labels\n",
    "    batch_labels = metadata[label_colname].values\n",
    "    unique_batches = np.unique(batch_labels)\n",
    "    n_batches = len(unique_batches)\n",
    "    \n",
    "    # Create mapping from batch to index\n",
    "    batch_to_idx = {batch: idx for idx, batch in enumerate(unique_batches)}\n",
    "    batch_indices = np.array([batch_to_idx[batch] for batch in batch_labels])\n",
    "    \n",
    "    # Find k-nearest neighbors (k should be larger than perplexity)\n",
    "    k = min(90, n_cells - 1)  # Use 90 neighbors or n_cells-1 if smaller\n",
    "    print(f\"Computing {k} nearest neighbors for {n_cells} cells...\")\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "    \n",
    "    lisi_scores = np.zeros(n_cells)\n",
    "    \n",
    "    # Add progress bar for LISI computation\n",
    "    print(f\"Computing LISI scores for {label_colname}...\")\n",
    "    for i in range(n_cells):\n",
    "        # Get neighbors and distances for current cell\n",
    "        neighbor_indices = indices[i, 1:]  # Exclude self (index 0)\n",
    "        neighbor_distances = distances[i, 1:]\n",
    "        \n",
    "        # Compute Gaussian kernel weights with adaptive bandwidth\n",
    "        # Find bandwidth that gives desired perplexity\n",
    "        sigma = find_sigma(neighbor_distances, perplexity)\n",
    "        weights = np.exp(-neighbor_distances**2 / (2 * sigma**2))\n",
    "        weights = weights / np.sum(weights)  # Normalize\n",
    "        \n",
    "        # Get batch labels of neighbors\n",
    "        neighbor_batches = batch_indices[neighbor_indices]\n",
    "        \n",
    "        # Compute probability of each batch in neighborhood\n",
    "        batch_probs = np.zeros(n_batches)\n",
    "        for j, batch_idx in enumerate(neighbor_batches):\n",
    "            batch_probs[batch_idx] += weights[j]\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        batch_probs = batch_probs + 1e-12\n",
    "        \n",
    "        # Compute Simpson diversity (inverse Simpson index)\n",
    "        simpson_index = np.sum(batch_probs**2)\n",
    "        lisi_scores[i] = 1.0 / simpson_index\n",
    "    \n",
    "    return lisi_scores\n",
    "\n",
    "def find_sigma(distances, target_perplexity, tol=1e-5, max_iter=50):\n",
    "    \"\"\"\n",
    "    Find the Gaussian kernel bandwidth (sigma) that achieves target perplexity.\n",
    "    Uses binary search similar to t-SNE implementation.\n",
    "    \"\"\"\n",
    "    def perplexity_fn(sigma):\n",
    "        if sigma <= 0:\n",
    "            return 0\n",
    "        weights = np.exp(-distances**2 / (2 * sigma**2))\n",
    "        weights = weights / np.sum(weights)\n",
    "        # Avoid log(0)\n",
    "        weights = np.maximum(weights, 1e-12)\n",
    "        H = -np.sum(weights * np.log2(weights))\n",
    "        return 2**H\n",
    "    \n",
    "    # Binary search for sigma\n",
    "    sigma_min, sigma_max = 1e-20, 1000.0\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        sigma = (sigma_min + sigma_max) / 2.0\n",
    "        perp = perplexity_fn(sigma)\n",
    "        \n",
    "        if abs(perp - target_perplexity) < tol:\n",
    "            break\n",
    "            \n",
    "        if perp > target_perplexity:\n",
    "            sigma_max = sigma\n",
    "        else:\n",
    "            sigma_min = sigma\n",
    "    \n",
    "    return sigma\n",
    "\n",
    "def ilisi_graph(adata, batch_key, type=\"embed\", use_rep=\"X_pca\", perplexity=30):\n",
    "    \"\"\"\n",
    "    Compute integration Local Inverse Simpson Index (iLISI) for an AnnData object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Annotated data object\n",
    "    batch_key : str\n",
    "        Key in adata.obs containing batch information\n",
    "    type : str, default=\"embed\"\n",
    "        Type of data to use (\"embed\" for embeddings)\n",
    "    use_rep : str, default=\"X_pca\"\n",
    "        Key in adata.obsm for the embedding to use\n",
    "    perplexity : int, default=30\n",
    "        Perplexity parameter for neighborhood definition\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Normalized mean iLISI score across all cells (0-1 range)\n",
    "    \"\"\"\n",
    "    if type == \"embed\":\n",
    "        print(\"Using embed\")\n",
    "        if use_rep not in adata.obsm:\n",
    "            raise ValueError(f\"Embedding {use_rep} not found in adata.obsm\")\n",
    "        X = adata.obsm[use_rep]\n",
    "    else:\n",
    "        X = adata.X\n",
    "    \n",
    "    if batch_key not in adata.obs:\n",
    "        raise ValueError(f\"Batch key {batch_key} not found in adata.obs\")\n",
    "    \n",
    "    # Get number of unique batches for normalization\n",
    "    n_batches = len(adata.obs[batch_key].unique())\n",
    "    \n",
    "    # Compute LISI scores\n",
    "    print(\"Computing LISI\")\n",
    "    lisi_scores = compute_lisi(X, adata.obs, batch_key, perplexity)\n",
    "    \n",
    "    # Normalize by number of batches (perfect mixing = 1.0, no mixing = 1/n_batches)\n",
    "    normalized_scores = (lisi_scores - 1) / (n_batches - 1)\n",
    "    \n",
    "    # Return mean normalized iLISI score\n",
    "    return np.mean(normalized_scores)\n",
    "\n",
    "def clisi_graph(adata, label_key, type=\"embed\", use_rep=\"X_pca\", perplexity=30):\n",
    "    \"\"\"\n",
    "    Compute cell-type Local Inverse Simpson Index (cLISI) for an AnnData object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Annotated data object\n",
    "    label_key : str\n",
    "        Key in adata.obs containing cell type information\n",
    "    type : str, default=\"embed\"\n",
    "        Type of data to use (\"embed\" for embeddings)\n",
    "    use_rep : str, default=\"X_pca\"\n",
    "        Key in adata.obsm for the embedding to use\n",
    "    perplexity : int, default=30\n",
    "        Perplexity parameter for neighborhood definition\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Normalized mean cLISI score across all cells (0-1 range)\n",
    "    \"\"\"\n",
    "    if type == \"embed\":\n",
    "        print(\"Using embed\")\n",
    "        if use_rep not in adata.obsm:\n",
    "            raise ValueError(f\"Embedding {use_rep} not found in adata.obsm\")\n",
    "        X = adata.obsm[use_rep]\n",
    "    else:\n",
    "        X = adata.X\n",
    "    \n",
    "    if label_key not in adata.obs:\n",
    "        raise ValueError(f\"Label key {label_key} not found in adata.obs\")\n",
    "    \n",
    "    # Get number of unique cell types for normalization\n",
    "    n_celltypes = len(adata.obs[label_key].unique())\n",
    "    \n",
    "    print(\"Computing LISI\")\n",
    "    # Compute LISI scores\n",
    "    lisi_scores = compute_lisi(X, adata.obs, label_key, perplexity)\n",
    "    \n",
    "    # Normalize by number of cell types (perfect mixing = 1.0, no mixing = 1/n_celltypes)\n",
    "    normalized_scores = (lisi_scores - 1) / (n_celltypes - 1)\n",
    "    \n",
    "    # Return mean normalized cLISI score\n",
    "    return np.mean(normalized_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the torch random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/workspaces/data/human_pancreas_norm_complexBatch.h5ad\")\n",
    "\n",
    "adata.raw = adata\n",
    "adata.layers[\"counts\"] = adata.X.copy()\n",
    "sc.pp.filter_cells(adata, min_genes=300)\n",
    "sc.pp.filter_genes(adata, min_cells=5)\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key='tech')\n",
    "adata = adata[:, adata.var['highly_variable']]\n",
    "multi_resolution_cluster(adata, resolution1 = 1, method = 'Leiden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_by_technology(adata, batch_key='tech', color_key='celltype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_integration_model(adata, batch_key = 'tech', z_dim=256, d_coef = 0.01, epochs=100, critic=True, disc_iter=10)\n",
    "obtain_embeddings(adata, model.to(\"cuda:0\"))\n",
    "sc.pp.neighbors(adata, use_rep=\"X_scCRAFT\")\n",
    "plot_umap_by_technology(adata, batch_key='tech', color_key='celltype')\n",
    "\n",
    "print(scib.me.silhouette(adata, label_key=\"celltype\", embed=\"X_scCRAFT\", scale=True))\n",
    "print(scib.me.silhouette_batch(adata, batch_key=\"tech\", label_key=\"celltype\", embed=\"X_scCRAFT\", scale=True))\n",
    "\n",
    "ilisi_score = ilisi_graph(adata, batch_key=\"tech\", type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "print(f\"iLISI score (1 is best): {ilisi_score:.4f}\")\n",
    "\n",
    "clisi_score = clisi_graph(adata, label_key=\"celltype\", type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "print(f\"cLISI score (0 is best): {clisi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_integration_model(adata, batch_key = 'tech', z_dim=256, d_coef = 0.1, epochs=100, critic=False, disc_iter=10)\n",
    "obtain_embeddings(adata, model.to(\"cuda:0\"))\n",
    "sc.pp.neighbors(adata, use_rep=\"X_scCRAFT\")\n",
    "plot_umap_by_technology(adata, batch_key='tech', color_key='celltype')\n",
    "\n",
    "print(scib.me.silhouette(adata, label_key=\"celltype\", embed=\"X_scCRAFT\", scale=True))\n",
    "print(scib.me.silhouette_batch(adata, batch_key=\"tech\", label_key=\"celltype\", embed=\"X_scCRAFT\", scale=True))\n",
    "\n",
    "ilisi_score = ilisi_graph(adata, batch_key=\"tech\", type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "print(f\"iLISI score (1 is best): {ilisi_score:.4f}\")\n",
    "\n",
    "clisi_score = clisi_graph(adata, label_key=\"celltype\", type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "print(f\"cLISI score (0 is best): {clisi_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def nested_cv_hyperparameter_tuning(adata, batch_key='tech', celltype_key='celltype', \n",
    "                                   d_coef_range=[0.01, 0.05, 0.1, 0.2, 0.5], \n",
    "                                   n_outer_folds=5, n_inner_folds=3, \n",
    "                                   z_dim=256, epochs=50, disc_iter=10, \n",
    "                                   random_state=42):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation for hyperparameter tuning of d_coef with and without critic.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        Annotated data object\n",
    "    batch_key : str\n",
    "        Key for batch information\n",
    "    celltype_key : str  \n",
    "        Key for cell type information\n",
    "    d_coef_range : list\n",
    "        Range of d_coef values to test\n",
    "    n_outer_folds : int\n",
    "        Number of outer CV folds\n",
    "    n_inner_folds : int\n",
    "        Number of inner CV folds for hyperparameter selection\n",
    "    z_dim : int\n",
    "        Latent dimension\n",
    "    epochs : int\n",
    "        Training epochs\n",
    "    disc_iter : int\n",
    "        Discriminator iterations\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : DataFrame\n",
    "        Results with outer fold statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    set_seed(random_state)\n",
    "    \n",
    "    # Prepare data indices for stratified splitting by cell type\n",
    "    cell_indices = np.arange(adata.n_obs)\n",
    "    celltype_labels = adata.obs[celltype_key].values\n",
    "    \n",
    "    # Create stratified outer folds\n",
    "    outer_kf = KFold(n_splits=n_outer_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Store results for each outer fold\n",
    "    outer_fold_results = {\n",
    "        'critic': {'ilisi': [], 'clisi': [], 'best_d_coef': []},\n",
    "        'no_critic': {'ilisi': [], 'clisi': [], 'best_d_coef': []}\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting nested {n_outer_folds}-fold CV with {len(d_coef_range)} d_coef values...\")\n",
    "    print(f\"d_coef range: {d_coef_range}\")\n",
    "    \n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_kf.split(cell_indices)):\n",
    "        print(f\"\\n=== OUTER FOLD {outer_fold + 1}/{n_outer_folds} ===\")\n",
    "        \n",
    "        # Split data for outer fold\n",
    "        adata_train = adata[train_idx].copy()\n",
    "        adata_test = adata[test_idx].copy()\n",
    "        \n",
    "        print(f\"Train set: {adata_train.n_obs} cells, Test set: {adata_test.n_obs} cells\")\n",
    "        \n",
    "        # Inner CV for hyperparameter selection (both critic and no_critic)\n",
    "        for use_critic in [True, False]:\n",
    "            critic_label = 'critic' if use_critic else 'no_critic'\n",
    "            print(f\"\\n--- {critic_label.upper()} ---\")\n",
    "            \n",
    "            # Inner cross-validation for hyperparameter selection\n",
    "            inner_kf = KFold(n_splits=n_inner_folds, shuffle=True, random_state=random_state)\n",
    "            inner_scores = defaultdict(list)\n",
    "            \n",
    "            print(\"Inner CV for hyperparameter selection...\")\n",
    "            for d_coef in d_coef_range:\n",
    "                print(f\"  Testing d_coef={d_coef}\")\n",
    "                \n",
    "                # Inner fold validation scores\n",
    "                inner_ilisi_scores = []\n",
    "                inner_clisi_scores = []\n",
    "                \n",
    "                for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(inner_kf.split(train_idx)):\n",
    "                    # Get actual indices\n",
    "                    actual_train_idx = train_idx[inner_train_idx]\n",
    "                    actual_val_idx = train_idx[inner_val_idx]\n",
    "                    \n",
    "                    adata_inner_train = adata[actual_train_idx].copy()\n",
    "                    adata_inner_val = adata[actual_val_idx].copy()\n",
    "                    \n",
    "                    try:\n",
    "                        # Train model on inner training set\n",
    "                        model = train_integration_model(\n",
    "                            adata_inner_train, \n",
    "                            batch_key=batch_key,\n",
    "                            z_dim=z_dim, \n",
    "                            d_coef=d_coef, \n",
    "                            epochs=epochs, \n",
    "                            critic=use_critic, \n",
    "                            disc_iter=disc_iter\n",
    "                        )\n",
    "                        \n",
    "                        # Get embeddings for validation set\n",
    "                        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "                        obtain_embeddings(adata_inner_val, model.to(device))\n",
    "                        \n",
    "                        # Compute validation scores\n",
    "                        ilisi_val = ilisi_graph(adata_inner_val, batch_key=batch_key, \n",
    "                                              type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "                        clisi_val = clisi_graph(adata_inner_val, label_key=celltype_key, \n",
    "                                              type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "                        \n",
    "                        inner_ilisi_scores.append(ilisi_val)\n",
    "                        inner_clisi_scores.append(clisi_val)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error in inner fold {inner_fold}: {e}\")\n",
    "                        inner_ilisi_scores.append(0.0)\n",
    "                        inner_clisi_scores.append(1.0)\n",
    "                \n",
    "                # Average scores across inner folds\n",
    "                avg_ilisi = np.mean(inner_ilisi_scores)\n",
    "                avg_clisi = np.mean(inner_clisi_scores)\n",
    "                \n",
    "                # Composite score (higher iLISI is better, lower cLISI is better)\n",
    "                composite_score = avg_ilisi - avg_clisi\n",
    "                \n",
    "                inner_scores[d_coef] = {\n",
    "                    'ilisi': avg_ilisi,\n",
    "                    'clisi': avg_clisi, \n",
    "                    'composite': composite_score\n",
    "                }\n",
    "                \n",
    "                print(f\"    d_coef={d_coef}: iLISI={avg_ilisi:.4f}, cLISI={avg_clisi:.4f}, composite={composite_score:.4f}\")\n",
    "            \n",
    "            # Select best hyperparameter based on composite score\n",
    "            best_d_coef = max(inner_scores.keys(), key=lambda k: inner_scores[k]['composite'])\n",
    "            print(f\"  Best d_coef for {critic_label}: {best_d_coef}\")\n",
    "            \n",
    "            # Train final model on full training set with best hyperparameter\n",
    "            print(f\"  Training final model with d_coef={best_d_coef}\")\n",
    "            try:\n",
    "                final_model = train_integration_model(\n",
    "                    adata_train, \n",
    "                    batch_key=batch_key,\n",
    "                    z_dim=z_dim, \n",
    "                    d_coef=best_d_coef, \n",
    "                    epochs=epochs, \n",
    "                    critic=use_critic, \n",
    "                    disc_iter=disc_iter\n",
    "                )\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "                obtain_embeddings(adata_test, final_model.to(device))\n",
    "                \n",
    "                # Compute test scores\n",
    "                test_ilisi = ilisi_graph(adata_test, batch_key=batch_key, \n",
    "                                       type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "                test_clisi = clisi_graph(adata_test, label_key=celltype_key, \n",
    "                                       type=\"embed\", use_rep=\"X_scCRAFT\")\n",
    "                \n",
    "                print(f\"  Test scores: iLISI={test_ilisi:.4f}, cLISI={test_clisi:.4f}\")\n",
    "                \n",
    "                # Store results\n",
    "                outer_fold_results[critic_label]['ilisi'].append(test_ilisi)\n",
    "                outer_fold_results[critic_label]['clisi'].append(test_clisi)\n",
    "                outer_fold_results[critic_label]['best_d_coef'].append(best_d_coef)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error in final training: {e}\")\n",
    "                outer_fold_results[critic_label]['ilisi'].append(0.0)\n",
    "                outer_fold_results[critic_label]['clisi'].append(1.0)\n",
    "                outer_fold_results[critic_label]['best_d_coef'].append(best_d_coef)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_data = []\n",
    "    for fold in range(n_outer_folds):\n",
    "        for critic_type in ['critic', 'no_critic']:\n",
    "            results_data.append({\n",
    "                'fold': fold + 1,\n",
    "                'method': critic_type,\n",
    "                'ilisi': outer_fold_results[critic_type]['ilisi'][fold],\n",
    "                'clisi': outer_fold_results[critic_type]['clisi'][fold],\n",
    "                'best_d_coef': outer_fold_results[critic_type]['best_d_coef'][fold]\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    return results_df, outer_fold_results\n",
    "\n",
    "def analyze_cv_results(results_df, outer_fold_results):\n",
    "    \"\"\"\n",
    "    Analyze and compare CV results with statistical tests.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NESTED CROSS-VALIDATION RESULTS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary_stats = results_df.groupby('method').agg({\n",
    "        'ilisi': ['mean', 'std', 'min', 'max'],\n",
    "        'clisi': ['mean', 'std', 'min', 'max'],\n",
    "        'best_d_coef': lambda x: list(x)\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\nSUMMARY STATISTICS:\")\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Statistical tests\n",
    "    critic_ilisi = outer_fold_results['critic']['ilisi']\n",
    "    no_critic_ilisi = outer_fold_results['no_critic']['ilisi']\n",
    "    critic_clisi = outer_fold_results['critic']['clisi']\n",
    "    no_critic_clisi = outer_fold_results['no_critic']['clisi']\n",
    "    \n",
    "    print(f\"\\nSTATISTICAL TESTS (n={len(critic_ilisi)} folds each):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Paired t-test for iLISI (higher is better)\n",
    "    ilisi_tstat, ilisi_pval = stats.ttest_rel(critic_ilisi, no_critic_ilisi)\n",
    "    print(f\"iLISI Paired t-test:\")\n",
    "    print(f\"  Critic mean  std: {np.mean(critic_ilisi):.4f}  {np.std(critic_ilisi):.4f}\")\n",
    "    print(f\"  No-critic mean  std: {np.mean(no_critic_ilisi):.4f}  {np.std(no_critic_ilisi):.4f}\")\n",
    "    print(f\"  t-statistic: {ilisi_tstat:.4f}\")\n",
    "    print(f\"  p-value: {ilisi_pval:.6f}\")\n",
    "    print(f\"  Significant: {'Yes' if ilisi_pval < 0.05 else 'No'}\")\n",
    "    if ilisi_pval < 0.05:\n",
    "        better_ilisi = \"Critic\" if np.mean(critic_ilisi) > np.mean(no_critic_ilisi) else \"No-critic\"\n",
    "        print(f\"  {better_ilisi} performs significantly better for iLISI\")\n",
    "    \n",
    "    # Paired t-test for cLISI (lower is better)\n",
    "    clisi_tstat, clisi_pval = stats.ttest_rel(critic_clisi, no_critic_clisi)\n",
    "    print(f\"\\ncLISI Paired t-test:\")\n",
    "    print(f\"  Critic mean  std: {np.mean(critic_clisi):.4f}  {np.std(critic_clisi):.4f}\")\n",
    "    print(f\"  No-critic mean  std: {np.mean(no_critic_clisi):.4f}  {np.std(no_critic_clisi):.4f}\")\n",
    "    print(f\"  t-statistic: {clisi_tstat:.4f}\")\n",
    "    print(f\"  p-value: {clisi_pval:.6f}\")\n",
    "    print(f\"  Significant: {'Yes' if clisi_pval < 0.05 else 'No'}\")\n",
    "    if clisi_pval < 0.05:\n",
    "        better_clisi = \"Critic\" if np.mean(critic_clisi) < np.mean(no_critic_clisi) else \"No-critic\"\n",
    "        print(f\"  {better_clisi} performs significantly better for cLISI\")\n",
    "    \n",
    "    # Effect sizes (Cohen's d)\n",
    "    def cohens_d(x, y):\n",
    "        nx, ny = len(x), len(y)\n",
    "        dof = nx + ny - 2\n",
    "        pooled_std = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / dof)\n",
    "        return (np.mean(x) - np.mean(y)) / pooled_std\n",
    "    \n",
    "    ilisi_cohens_d = cohens_d(critic_ilisi, no_critic_ilisi)\n",
    "    clisi_cohens_d = cohens_d(no_critic_clisi, critic_clisi)  # Flipped for \"lower is better\"\n",
    "    \n",
    "    print(f\"\\nEFFECT SIZES (Cohen's d):\")\n",
    "    print(f\"  iLISI Cohen's d: {ilisi_cohens_d:.4f}\")\n",
    "    print(f\"  cLISI Cohen's d: {clisi_cohens_d:.4f}\")\n",
    "    \n",
    "    def interpret_cohens_d(d):\n",
    "        if abs(d) < 0.2:\n",
    "            return \"negligible\"\n",
    "        elif abs(d) < 0.5:\n",
    "            return \"small\"\n",
    "        elif abs(d) < 0.8:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"large\"\n",
    "    \n",
    "    print(f\"  iLISI effect size: {interpret_cohens_d(ilisi_cohens_d)}\")\n",
    "    print(f\"  cLISI effect size: {interpret_cohens_d(clisi_cohens_d)}\")\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric alternative)\n",
    "    ilisi_wilc_stat, ilisi_wilc_pval = stats.wilcoxon(critic_ilisi, no_critic_ilisi)\n",
    "    clisi_wilc_stat, clisi_wilc_pval = stats.wilcoxon(critic_clisi, no_critic_clisi)\n",
    "    \n",
    "    print(f\"\\nWILCOXON SIGNED-RANK TESTS (non-parametric):\")\n",
    "    print(f\"  iLISI p-value: {ilisi_wilc_pval:.6f}\")\n",
    "    print(f\"  cLISI p-value: {clisi_wilc_pval:.6f}\")\n",
    "    \n",
    "    # Hyperparameter frequency analysis\n",
    "    print(f\"\\nHYPERPARAMETER SELECTION FREQUENCY:\")\n",
    "    critic_d_coef_freq = pd.Series(outer_fold_results['critic']['best_d_coef']).value_counts()\n",
    "    no_critic_d_coef_freq = pd.Series(outer_fold_results['no_critic']['best_d_coef']).value_counts()\n",
    "    \n",
    "    print(\"Critic:\")\n",
    "    for d_coef, freq in critic_d_coef_freq.items():\n",
    "        print(f\"  d_coef={d_coef}: {freq}/{len(critic_ilisi)} folds\")\n",
    "    \n",
    "    print(\"No-critic:\")\n",
    "    for d_coef, freq in no_critic_d_coef_freq.items():\n",
    "        print(f\"  d_coef={d_coef}: {freq}/{len(no_critic_ilisi)} folds\")\n",
    "    \n",
    "    # Overall conclusion\n",
    "    print(f\"\\nOVERALL CONCLUSION:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Count significant improvements\n",
    "    significant_improvements = 0\n",
    "    if ilisi_pval < 0.05 and np.mean(critic_ilisi) > np.mean(no_critic_ilisi):\n",
    "        significant_improvements += 1\n",
    "        print(\" Critic significantly better for batch mixing (iLISI)\")\n",
    "    elif ilisi_pval < 0.05:\n",
    "        print(\" No-critic significantly better for batch mixing (iLISI)\")\n",
    "    else:\n",
    "        print(\" No significant difference in batch mixing (iLISI)\")\n",
    "    \n",
    "    if clisi_pval < 0.05 and np.mean(critic_clisi) < np.mean(no_critic_clisi):\n",
    "        significant_improvements += 1\n",
    "        print(\" Critic significantly better for cell type preservation (cLISI)\")\n",
    "    elif clisi_pval < 0.05:\n",
    "        print(\" No-critic significantly better for cell type preservation (cLISI)\")\n",
    "    else:\n",
    "        print(\" No significant difference in cell type preservation (cLISI)\")\n",
    "    \n",
    "    if significant_improvements == 2:\n",
    "        print(\"\\n CRITIC APPROACH SHOWS SIGNIFICANT IMPROVEMENT IN BOTH METRICS!\")\n",
    "    elif significant_improvements == 1:\n",
    "        print(\"\\n  CRITIC APPROACH SHOWS MIXED RESULTS (better in one metric)\")\n",
    "    else:\n",
    "        print(\"\\n CRITIC APPROACH DOES NOT SHOW SIGNIFICANT IMPROVEMENT\")\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "print(\"Starting nested cross-validation...\")\n",
    "results_df, outer_fold_results = nested_cv_hyperparameter_tuning(\n",
    "    adata,\n",
    "    batch_key='tech',\n",
    "    celltype_key='celltype', \n",
    "    d_coef_range=[0.01, 0.05, 0.1, 0.2],  # Reduced range for faster execution\n",
    "    n_outer_folds=5,\n",
    "    n_inner_folds=3,\n",
    "    z_dim=128,      # Reduced for faster training\n",
    "    epochs=30,      # Reduced for faster training  \n",
    "    disc_iter=5,    # Reduced for faster training\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "summary_stats = analyze_cv_results(results_df, outer_fold_results)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('nested_cv_results.csv', index=False)\n",
    "print(f\"\\nResults saved to 'nested_cv_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
